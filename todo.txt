##TODO: [X] Available performance metrics
##TODO: [X] Finish to write data config files
##TODO: [X] I delet gitignore file from mtl-baseline folder and add it to the root folder
##TODO: [X] check if OSACT2022 dataset with labels in two different columns will work with mtl-baseline
##TODO: [X] make necessary changes to utils_two.py
##TODO: [X] preper GPU env
##TODO: [X] generate new  .yml and .requirements.txt files after make mlt-baseline working

##TODO: [X] set parameter for first experiment (check paper file google docs)
##TODO: [X] fix Preprocessing @USER and @URL
##TODO: [X] create a tmux session for running the first experiment
##TODO: [X] run small pillot experiment

##BUG: [X] inconsistenci OSACT train and dev dataset -> 7395 jump 7795 & -> 9698 jump 9703
##BUG: [X] preprocessinf OSACT dataset by mtl-baseline (experiment_damiano)
##BUG: [X] OSACT overload GPU memory
##BUG: [X] remove debug.py

##TODO: [X] set epochs to 15
##TODO: [X] run experiment

##TODO: [X] improve Analyses.py script (it generate confidence intervals)
##TODO: [x] change script name experiment_damiano.py to train-test.py or sth else
##TODO: [x] change script name experiment_two.py to cross-validation.py or sth else
##TODO: [X] check the overall code before start with the mtl task ware models

##TODO: [X] check the script for cross-validation of the baselines
##TODO: [-] preper code to call Transformer localy instead from HuggingFace -> not possible machamp does not support
##TODO: [-] I may modify code two run in two GPUs -> machamp does not allow due to AlleNLP constrains
##TODO: [X] add repo to a second remote machine
##TODO: [X] run a small test for cross-validation of the baselines
##TODO: [X] plan version countrol in all this machines 
##TODO: [X] run cross-validation for the baselines

##TODO: [X] collect results for cross-validation of the baselines 
##TODO: [X] Add results to the overleaf on a Table

##TODO: [X] add GPT inside vs code
##TODO: [-] add chrome inside vs code

##TODO: [X] start with the mtl Task Awareness models
##TODO: [ ] mtl-model: check scripts
            - [X] analysis.py
            - [X] train-test.py 
            - [X] gridsearch.py
            - [X] grid_dataparallelism.py
            - [X] duplicate_samples.py 
            - [X] dataset.py
            - [X] data.py
            - [X] model.py
            - [x] model_dataparallelism.py

##TODO: [X] mtl-model: test with original data
##TODO: [X] Unify parallel script and not parallel scripts
            - [X] train-test.py 
            - [X] gridsearch.py
            - [X] grid_dataparallelism.py
            - [X] model.py
            - [X] model_dataparallelism.py (out.get_device())

##TODO: [X] mtl-model: How to caculate confidence intervals?
##TODO: [ ] mtl-model: adapt scripts
            - [X] train-test.py 
                  - [X] change the name of the class  or if it is the same cv classs remoce it from the script and call it
                  - [-] train-test.py -> it should run on the best parameter from gridsearch
                  - [X] train-test must save results (in log folder) as 'train-test' not gridsearch
                  - [X] change the name of the script gred_search to CrossValidation.
            - [X] movie gitignore to new gitignore
            - [ ] I may remove end file .yml
            - [ ] remove def download_data from data.py
            - [ ] remove comments from all scripts
            - [ ] get rid of necessary folders and files
            
##TODO: [ ] mtl-model: How to get best results?
##TODO: [ ] change env name if it work for the two MLT models (base and mine)
##TODO: [ ] medium scale test using original data
##TODO: [ ] adapt the code for the arabic data